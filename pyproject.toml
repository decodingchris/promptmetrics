[project]
name = "promptmetrics"
version = "0.1.0"
description = "A modular toolkit for the rigorous evaluation and metric generation of LLM prompts."
dependencies = [
    "datasets",
    "openai",
    "python-dotenv",
    "tqdm",
    "Pillow",
    "pydantic",
    "httpx",
    "numpy",
]
readme = "README.md"
requires-python = ">= 3.11"

# Defines clean command-line entry points
[project.scripts]
pm-generate = "promptmetrics.scripts.run_generation:main"
pm-evaluate = "promptmetrics.scripts.run_evaluation:main"

# Defines dependencies only needed for development and testing
[project.optional-dependencies]
dev = [
    "pre-commit",
    "pytest",
    "pytest-cov",
    "pytest-mock",
    "pytest-asyncio",
    "ruff",
    "mypy",
    ""types-tqdm",
]

[tool.uv]
package = true

[tool.coverage.run]
source = ["src/promptmetrics"]
omit = [
    # Exclude the abstract base class, which is not directly testable
    "src/promptmetrics/benchmarks/base.py",
    # Exclude the empty __init__.py file
    "src/promptmetrics/__init__.py",
]

[tool.mypy]
# Ignore untyped 3rd party libraries that don't have stubs
[[tool.mypy.overrides]]
module = ["datasets", "datasets.*"]
ignore_missing_imports = true
